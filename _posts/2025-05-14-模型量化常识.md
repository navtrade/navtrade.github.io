---
title: "模型量化常识"
date: 2025-05-14
---

# PTQ

Post-Trainning Quantization

- GPTQ
- AWQ

# QAT

Quantization-Aware Trainning

当通过 PTQ（甚至是先进的方法）实现的精度不能满足您的应用要求时，尤其是在针对激进的低位量化时，量化感知训练 (QAT) 就变得必要。

QAT 与 PTQ 有着根本的不同，因为它在训练或微调过程中引入了量化的影响。QAT 并非对针对浮点优化的模型进行量化，而是帮助模型学习对量化引入的噪声和信息损失具有更强鲁棒性的权重。本质上，它允许模型在学习过程中“适应”低精度算法的限制。

可以这样想：PTQ 就像让一位在理想条件下训练的运动员穿着次优装备进行比赛。他们或许会有所适应，但最佳表现可能会受到影响。QAT 就像从一开始就（或在特定的体能训练阶段）用运动员实际使用的装备进行训练，让他们能够针对这些限制条件优化技术和力量。